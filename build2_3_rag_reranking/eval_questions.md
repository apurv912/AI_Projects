Which env vars configure the models?

What is retrieve-only mode and why use it?

What happens when answer generation hits 429?

What happens when rerank hits 429?

Which file documents the “trust layer” (citations + validator)?

List the three model env vars used in this app.

Which behavior prevents rerank from wasting quota with small candidate sets?

How does the similarity guardrail affect cost?

Where are the retrieved sources shown in the UI?

What does “rerank debug (stage1 vs final)” help you verify?